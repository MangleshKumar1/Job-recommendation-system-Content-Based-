{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-18T08:48:55.562263Z",
     "iopub.status.busy": "2024-04-18T08:48:55.561911Z",
     "iopub.status.idle": "2024-04-18T08:48:56.417648Z",
     "shell.execute_reply": "2024-04-18T08:48:56.416668Z",
     "shell.execute_reply.started": "2024-04-18T08:48:55.562234Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "#import os\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "   # for filename in filenames:\n",
    "     #   print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:48:56.419622Z",
     "iopub.status.busy": "2024-04-18T08:48:56.419241Z",
     "iopub.status.idle": "2024-04-18T08:48:59.965552Z",
     "shell.execute_reply": "2024-04-18T08:48:59.964606Z",
     "shell.execute_reply.started": "2024-04-18T08:48:56.419597Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/job-recommendation-datasets/Combined_Jobs_Final.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/job-recommendation-datasets/Combined_Jobs_Final.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/job-recommendation-datasets/Combined_Jobs_Final.csv'"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv('/kaggle/input/job-recommendation-datasets/Combined_Jobs_Final.csv')\n",
    "df.head(3) # understanding first 3 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:48:59.967326Z",
     "iopub.status.busy": "2024-04-18T08:48:59.966933Z",
     "iopub.status.idle": "2024-04-18T08:49:00.109493Z",
     "shell.execute_reply": "2024-04-18T08:49:00.108413Z",
     "shell.execute_reply.started": "2024-04-18T08:48:59.967290Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info() # info that what all columns are about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:49:00.111943Z",
     "iopub.status.busy": "2024-04-18T08:49:00.111667Z",
     "iopub.status.idle": "2024-04-18T08:49:00.126739Z",
     "shell.execute_reply": "2024-04-18T08:49:00.125797Z",
     "shell.execute_reply.started": "2024-04-18T08:49:00.111919Z"
    }
   },
   "outputs": [],
   "source": [
    "# We will use Content-based recommendation system :- It uses tags, descriptions\n",
    "# we will be using only 2 columns Title, Job Description column.\n",
    "df_old=df[['Title','Job.Description']]\n",
    "df_old # df dataframe will be our new dataset and we will be working on this dataset only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:49:00.128163Z",
     "iopub.status.busy": "2024-04-18T08:49:00.127848Z",
     "iopub.status.idle": "2024-04-18T08:49:00.136490Z",
     "shell.execute_reply": "2024-04-18T08:49:00.135780Z",
     "shell.execute_reply.started": "2024-04-18T08:49:00.128131Z"
    }
   },
   "outputs": [],
   "source": [
    "# can reduce no. of rows so that it can work on hardware\n",
    "#df=df_old.sample(n=1000,random_state=0)  # out of 84089 rows will only consider  1000 rows\n",
    "# will get a new different set of DataFrame with 1000 random rows Every time when running the code\n",
    "#df=df.iloc[:10000, :]\n",
    "df=df_old.head(10000).copy()   # Using head for selection, then copy into new dataframe, so that it don't create error in applying apply() at later point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:49:00.137929Z",
     "iopub.status.busy": "2024-04-18T08:49:00.137558Z",
     "iopub.status.idle": "2024-04-18T08:49:00.149730Z",
     "shell.execute_reply": "2024-04-18T08:49:00.148890Z",
     "shell.execute_reply.started": "2024-04-18T08:49:00.137895Z"
    }
   },
   "outputs": [],
   "source": [
    "# will need to explore both column one by one\n",
    "# as these two columns are \"Text\" and  \n",
    "# our machine can't understand text so first of all we need to clean our dataset\n",
    "df['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:49:00.151029Z",
     "iopub.status.busy": "2024-04-18T08:49:00.150778Z",
     "iopub.status.idle": "2024-04-18T08:49:00.158841Z",
     "shell.execute_reply": "2024-04-18T08:49:00.157899Z",
     "shell.execute_reply.started": "2024-04-18T08:49:00.151006Z"
    }
   },
   "outputs": [],
   "source": [
    "#lots of job Kitchen Staff,Book Keeper,...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:49:00.160472Z",
     "iopub.status.busy": "2024-04-18T08:49:00.159980Z",
     "iopub.status.idle": "2024-04-18T08:49:00.172746Z",
     "shell.execute_reply": "2024-04-18T08:49:00.171906Z",
     "shell.execute_reply.started": "2024-04-18T08:49:00.160442Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Title'][6558]  # row 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:49:00.174182Z",
     "iopub.status.busy": "2024-04-18T08:49:00.173925Z",
     "iopub.status.idle": "2024-04-18T08:49:00.184430Z",
     "shell.execute_reply": "2024-04-18T08:49:00.183580Z",
     "shell.execute_reply.started": "2024-04-18T08:49:00.174160Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Job.Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:49:00.189159Z",
     "iopub.status.busy": "2024-04-18T08:49:00.188861Z",
     "iopub.status.idle": "2024-04-18T08:49:00.198787Z",
     "shell.execute_reply": "2024-04-18T08:49:00.197682Z",
     "shell.execute_reply.started": "2024-04-18T08:49:00.189136Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Job.Description'][6558] # information about column 1 Job.Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:49:00.200591Z",
     "iopub.status.busy": "2024-04-18T08:49:00.200198Z",
     "iopub.status.idle": "2024-04-18T08:49:00.210109Z",
     "shell.execute_reply": "2024-04-18T08:49:00.209198Z",
     "shell.execute_reply.started": "2024-04-18T08:49:00.200560Z"
    }
   },
   "outputs": [],
   "source": [
    "# In this in each row in entire Description, we can find a lots of keywords in decription\n",
    "# we will first clean content of both column 'Title','Job.Description'\n",
    "# which means we will remove all other things other than text.\n",
    "# Like STOP WORDS, Special Characters , \\r\\n\\r\\n,  other clutters other than text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:49:00.211529Z",
     "iopub.status.busy": "2024-04-18T08:49:00.211208Z",
     "iopub.status.idle": "2024-04-18T08:49:01.945881Z",
     "shell.execute_reply": "2024-04-18T08:49:01.945084Z",
     "shell.execute_reply.started": "2024-04-18T08:49:00.211503Z"
    }
   },
   "outputs": [],
   "source": [
    "# We will make a function which will clean all these non-essential content from our Title and Description.\n",
    "\n",
    "import nltk # Natural Language Toolkit\n",
    "from nltk.stem.porter import PorterStemmer  # It is very useful algorithm \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re # regular expression library from python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:49:01.947299Z",
     "iopub.status.busy": "2024-04-18T08:49:01.946879Z",
     "iopub.status.idle": "2024-04-18T08:49:01.951437Z",
     "shell.execute_reply": "2024-04-18T08:49:01.950523Z",
     "shell.execute_reply.started": "2024-04-18T08:49:01.947253Z"
    }
   },
   "outputs": [],
   "source": [
    "# making object of PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "# Porter Stemmer is an algorithm used in natural language processing (NLP) for stemming words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:49:01.952752Z",
     "iopub.status.busy": "2024-04-18T08:49:01.952495Z",
     "iopub.status.idle": "2024-04-18T08:49:01.962057Z",
     "shell.execute_reply": "2024-04-18T08:49:01.961158Z",
     "shell.execute_reply.started": "2024-04-18T08:49:01.952729Z"
    }
   },
   "outputs": [],
   "source": [
    "# making a function to clean The text of Title and Description\n",
    "def cleaning(txt):\n",
    "    cleaned_txt = re.sub(r'[^a-zA-Z0-9\\s]', '', txt)\n",
    "    return cleaned_txt    \n",
    "    # we don't want \\n \\r and special characters, We want text and Integers only and other things will be removed\n",
    "    # we used Substract function  sub() of Regular Expression library(re)\n",
    "   # Inside that using \"\" or'' if content is other than i.e. (Negation of)^ a-z , A-Z or 0-9 and whitespace characters (\\s)\n",
    "# other than these if any other character encounters so don't include them and replace them with '' empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:49:01.963497Z",
     "iopub.status.busy": "2024-04-18T08:49:01.963162Z",
     "iopub.status.idle": "2024-04-18T08:49:01.974442Z",
     "shell.execute_reply": "2024-04-18T08:49:01.973453Z",
     "shell.execute_reply.started": "2024-04-18T08:49:01.963466Z"
    }
   },
   "outputs": [],
   "source": [
    "cleaning('This is my book with 0987 3 pages and digit  \\n]\\r\\t $@$%')\n",
    "# we seen above function removed ] and other special characters, but will not able to remove \\r\\n\\t,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:49:01.975886Z",
     "iopub.status.busy": "2024-04-18T08:49:01.975550Z",
     "iopub.status.idle": "2024-04-18T08:49:01.983824Z",
     "shell.execute_reply": "2024-04-18T08:49:01.983033Z",
     "shell.execute_reply.started": "2024-04-18T08:49:01.975857Z"
    }
   },
   "outputs": [],
   "source": [
    "# correcting this function to clean The text of Title and Description\n",
    "def cleaning(txt):\n",
    "    cleaned_txt = re.sub(r'[^a-zA-Z0-9\\s]', '', txt)\n",
    "    # we don't want \\n \\r, We want text and Integers only and other things will be removed\n",
    "    # we used Substract function  sub() of Regular Expression library(re)\n",
    "   # Inside that using \"\" or'' if content is other than i.e. (Negation of)^ a-z , A-Z or 0-9 \n",
    "# other than these if any other character encounters so don't include them and replace them with '' empty string.\n",
    "# But this can remove special characters, but will not able to remove \\r\\n\\t, we will tokenise\n",
    "    tokens= nltk.word_tokenize(cleaned_txt.lower())\n",
    "# we will tokenise(we will break word-by-word) and will also convert all characters into lower case\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:49:01.985023Z",
     "iopub.status.busy": "2024-04-18T08:49:01.984773Z",
     "iopub.status.idle": "2024-04-18T08:49:02.012029Z",
     "shell.execute_reply": "2024-04-18T08:49:02.011195Z",
     "shell.execute_reply.started": "2024-04-18T08:49:01.985002Z"
    }
   },
   "outputs": [],
   "source": [
    "cleaning('This is my book with 0987  3 pages and  digit \\n]\\r\\t $@$%')\n",
    "# tokenized all and converted to lower case, this will also remove \\n \\r \\t,.. in the process of tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:49:02.013367Z",
     "iopub.status.busy": "2024-04-18T08:49:02.013069Z",
     "iopub.status.idle": "2024-04-18T08:49:02.017632Z",
     "shell.execute_reply": "2024-04-18T08:49:02.016751Z",
     "shell.execute_reply.started": "2024-04-18T08:49:02.013344Z"
    }
   },
   "outputs": [],
   "source": [
    "# now we don't want tokens like 'this', 'is', 'my', 'with','and'\n",
    "# 'copy','0987', '3', 'pages' are good keyword can use\n",
    "\n",
    "# so will remove stopWords, Stopwords are the most frequently occurring words in a language \n",
    "# that carry little or no meaning on their own. They include words like \"the,\" \"a,\" \"an,\" \"is,\" \"of,\" \"and,\" etc.\n",
    "# and will use Stemming by  ps=PorterStemmer() function\n",
    "# Stemming aims to reduce words to their base or root form.\n",
    "#For example, \"running,\" \"runs,\" and \"ran\" would all be stemmed to \"run.\"\n",
    "# The goal is to group similar words together, focusing on their core meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:49:02.019566Z",
     "iopub.status.busy": "2024-04-18T08:49:02.019137Z",
     "iopub.status.idle": "2024-04-18T08:49:02.028928Z",
     "shell.execute_reply": "2024-04-18T08:49:02.028035Z",
     "shell.execute_reply.started": "2024-04-18T08:49:02.019536Z"
    }
   },
   "outputs": [],
   "source": [
    "# correcting this function to clean The text of Title and Description by also removing StemWords\n",
    "def cleaning(txt):\n",
    "    cleaned_txt = re.sub(r'[^a-zA-Z0-9\\s]', '', txt)\n",
    "    tokens= nltk.word_tokenize(cleaned_txt.lower())\n",
    "    # using list comprehension \n",
    "    # [for word in tokens if word not in stopwords.words('english')]\n",
    "    # let say tokens have 1,00,000 words so above line will run that number of time\n",
    "    #if these words are not from  then will take  those words in ps.stem() function\n",
    "    stemming = [ps.stem(word) for word in tokens if word not in stopwords.words('english')]\n",
    "\n",
    "    \n",
    "    return stemming    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:49:02.030128Z",
     "iopub.status.busy": "2024-04-18T08:49:02.029819Z",
     "iopub.status.idle": "2024-04-18T08:49:02.052798Z",
     "shell.execute_reply": "2024-04-18T08:49:02.051896Z",
     "shell.execute_reply.started": "2024-04-18T08:49:02.030100Z"
    }
   },
   "outputs": [],
   "source": [
    "cleaning('This is my book with 0987  3 pages and  digit moving driven \\n]\\r\\t $@$%')\n",
    "# [for word in tokens if word not in stopwords.words('english')] taken only those words who are not Stopwords\n",
    "# these are stored in word and then given to ps.stem(word)\n",
    "# this makes those non-stopwords into base form by help of ntlk PorterStemmer() function from nltk.stem.porter library by  import PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:49:02.054041Z",
     "iopub.status.busy": "2024-04-18T08:49:02.053792Z",
     "iopub.status.idle": "2024-04-18T08:49:02.057590Z",
     "shell.execute_reply": "2024-04-18T08:49:02.056651Z",
     "shell.execute_reply.started": "2024-04-18T08:49:02.054020Z"
    }
   },
   "outputs": [],
   "source": [
    "# it is returning a list,  returning it as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:49:02.059030Z",
     "iopub.status.busy": "2024-04-18T08:49:02.058765Z",
     "iopub.status.idle": "2024-04-18T08:49:02.067643Z",
     "shell.execute_reply": "2024-04-18T08:49:02.066808Z",
     "shell.execute_reply.started": "2024-04-18T08:49:02.059007Z"
    }
   },
   "outputs": [],
   "source": [
    "# correcting this function to clean The text of Title and Description by making change in return line\n",
    "def cleaning(txt):\n",
    "    cleaned_txt = re.sub(r'[^a-zA-Z0-9\\s]', '', txt)\n",
    "    tokens= nltk.word_tokenize(cleaned_txt.lower())\n",
    "    \n",
    "    stemming = [ps.stem(word) for word in tokens if word not in stopwords.words('english')]\n",
    "\n",
    "    \n",
    "    #return stemming    \n",
    "    return \" \".join(stemming) # joining tokens by separating them with \" \" space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:49:02.069079Z",
     "iopub.status.busy": "2024-04-18T08:49:02.068802Z",
     "iopub.status.idle": "2024-04-18T08:49:02.082168Z",
     "shell.execute_reply": "2024-04-18T08:49:02.081385Z",
     "shell.execute_reply.started": "2024-04-18T08:49:02.069057Z"
    }
   },
   "outputs": [],
   "source": [
    "cleaning('This is my book with 0987  3 pages and  digit moving driven \\n]\\r\\t $@$%')\n",
    "# converted into a sigle string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:49:02.083640Z",
     "iopub.status.busy": "2024-04-18T08:49:02.083316Z",
     "iopub.status.idle": "2024-04-18T08:49:14.557834Z",
     "shell.execute_reply": "2024-04-18T08:49:14.557077Z",
     "shell.execute_reply.started": "2024-04-18T08:49:02.083594Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now we seen that it can process on paragraph and find some keywords\n",
    "# now we will pass our title in this function\n",
    "df['Title']= df['Title'].apply( lambda x:cleaning(x))\n",
    "# as Title having 84089 so we will apply lambda, \n",
    "# and x will take each row of Title one by one, and will send it to cleaning() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can give error if our all the data is not of object type, but fortunately we have all data as object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:49:14.559181Z",
     "iopub.status.busy": "2024-04-18T08:49:14.558875Z",
     "iopub.status.idle": "2024-04-18T08:49:14.566793Z",
     "shell.execute_reply": "2024-04-18T08:49:14.565922Z",
     "shell.execute_reply.started": "2024-04-18T08:49:14.559154Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:49:14.568366Z",
     "iopub.status.busy": "2024-04-18T08:49:14.568002Z",
     "iopub.status.idle": "2024-04-18T08:55:06.265780Z",
     "shell.execute_reply": "2024-04-18T08:55:06.265006Z",
     "shell.execute_reply.started": "2024-04-18T08:49:14.568335Z"
    }
   },
   "outputs": [],
   "source": [
    "# df['Job.Description']= df['Job.Description'].apply( lambda x:cleaning(x))\n",
    "# above line gives error that  \"expected string or bytes-like object\"\n",
    "# so before apply() function we will convert our column into string type\n",
    "df['Job.Description']= df['Job.Description'].astype(str).apply( lambda x:cleaning(x))\n",
    "# using astype(str) to convert our column into string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:55:06.267035Z",
     "iopub.status.busy": "2024-04-18T08:55:06.266786Z",
     "iopub.status.idle": "2024-04-18T08:55:06.274641Z",
     "shell.execute_reply": "2024-04-18T08:55:06.273703Z",
     "shell.execute_reply.started": "2024-04-18T08:55:06.267013Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Job.Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:55:06.280663Z",
     "iopub.status.busy": "2024-04-18T08:55:06.280243Z",
     "iopub.status.idle": "2024-04-18T08:55:06.287548Z",
     "shell.execute_reply": "2024-04-18T08:55:06.286738Z",
     "shell.execute_reply.started": "2024-04-18T08:55:06.280636Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Job.Description'][6558] # now if we just see again row 1 does not contain unnecessary tokens or special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:55:06.289104Z",
     "iopub.status.busy": "2024-04-18T08:55:06.288786Z",
     "iopub.status.idle": "2024-04-18T08:55:06.310374Z",
     "shell.execute_reply": "2024-04-18T08:55:06.309460Z",
     "shell.execute_reply.started": "2024-04-18T08:55:06.289065Z"
    }
   },
   "outputs": [],
   "source": [
    "df['new_col']= df['Title'] +\" \" + df['Job.Description']\n",
    "# making a new 3rd column by joining the old two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:55:06.312411Z",
     "iopub.status.busy": "2024-04-18T08:55:06.311659Z",
     "iopub.status.idle": "2024-04-18T08:55:06.327212Z",
     "shell.execute_reply": "2024-04-18T08:55:06.326407Z",
     "shell.execute_reply.started": "2024-04-18T08:55:06.312375Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:55:06.328751Z",
     "iopub.status.busy": "2024-04-18T08:55:06.328431Z",
     "iopub.status.idle": "2024-04-18T08:55:06.335815Z",
     "shell.execute_reply": "2024-04-18T08:55:06.334823Z",
     "shell.execute_reply.started": "2024-04-18T08:55:06.328727Z"
    }
   },
   "outputs": [],
   "source": [
    "# now talking about vectorization\n",
    "# as we are maikng an algorithm for Recommendation system\n",
    "# machine can understand numbers and not text\n",
    "# so first we will try to make vector from it.\n",
    "\n",
    "# 2 things Vector , tfidfvectorizer\n",
    "# we find cosine angle between two vectors if it comes 0, so vectors are too similar and if it comes 90 then these are totally different \n",
    "# sckit-learn provides us cosine\n",
    "# but one more thing need to be done for these vectors-> tfidfvectorizer\n",
    "# tfidfvectorizer -> Term Frequency-Inverse Document Frequency Vectorizer\n",
    "# our row(84,000+) which is containing sentences are also called document.\n",
    "# tfidfvectorizer will help in calculating frequency of each word/token in entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:55:06.337680Z",
     "iopub.status.busy": "2024-04-18T08:55:06.337055Z",
     "iopub.status.idle": "2024-04-18T08:55:06.345310Z",
     "shell.execute_reply": "2024-04-18T08:55:06.344442Z",
     "shell.execute_reply.started": "2024-04-18T08:55:06.337647Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:55:06.346630Z",
     "iopub.status.busy": "2024-04-18T08:55:06.346310Z",
     "iopub.status.idle": "2024-04-18T08:55:16.612205Z",
     "shell.execute_reply": "2024-04-18T08:55:16.611123Z",
     "shell.execute_reply.started": "2024-04-18T08:55:06.346601Z"
    }
   },
   "outputs": [],
   "source": [
    "# object of TfidfVectorizer\n",
    "tfidf= TfidfVectorizer()\n",
    "#using fit_transform() function \n",
    "matrix=tfidf.fit_transform(df['new_col'])\n",
    "# out of these rows we will find most similar rows/vector by help of cosine_similarity given by scikit learn\n",
    "similarity= cosine_similarity(matrix)\n",
    "similarity  \n",
    "# this will return lists containing values such that it describes similarity of one row values with other remaining rows values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:55:16.613669Z",
     "iopub.status.busy": "2024-04-18T08:55:16.613355Z",
     "iopub.status.idle": "2024-04-18T08:55:16.622073Z",
     "shell.execute_reply": "2024-04-18T08:55:16.621184Z",
     "shell.execute_reply.started": "2024-04-18T08:55:16.613643Z"
    }
   },
   "outputs": [],
   "source": [
    "similarity[3]\n",
    "# this give a list showing similarity score all rows which are similar to this row 3 (may be similar tag, content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sorted(list(enumerate(similarity[idx])), key=lambda x:x[1] , reverse=False)\n",
    "\n",
    "**enumerate** is a built-in function in Python that takes an iterable (like a list) and returns an iterator that produces tuples.Each tuple contains two elements:\n",
    "* The first element is the index (position) of the item in the original iterable.\n",
    "* The second element is the item itself.\n",
    "\n",
    "**similarity[idx]** refers to a specific element (indexed by idx) within a list or array named similarity.\n",
    "It sorts the list of tuples created by enumerate(similarity[idx]) based on the similarity values (the second element in each tuple) in ascending order (reverse=False).\n",
    "\n",
    "**list(...)** converts the iterator returned by enumerate into a regular Python list. This list contains the tuples with indices and corresponding similarity values.\n",
    "\n",
    "**sorted()** function sorts the elements within the list. However, it requires a sorting criteria, which is specified by the key argument.\n",
    "\n",
    "**key=lambda x: x[1], reverse=False** \n",
    "     lambda x: x[1] is a **lambda function** that takes a single argument x (which represents one of the tuples in the list). It returns the second element (x[1]) of the tuple, which is the similarity value\n",
    "**reverse=False** sort the list based on the similarity values (second element) in ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:55:16.623442Z",
     "iopub.status.busy": "2024-04-18T08:55:16.623117Z",
     "iopub.status.idle": "2024-04-18T08:55:16.631730Z",
     "shell.execute_reply": "2024-04-18T08:55:16.630943Z",
     "shell.execute_reply.started": "2024-04-18T08:55:16.623416Z"
    }
   },
   "outputs": [],
   "source": [
    "# function which takes which takes a title input by user\n",
    "def recommendation(title):\n",
    "    idx= df[df['Title'] == title].index[0]  # storing index or rows which having same Title as given by user(title)\n",
    "    #idx=df.index.get_loc(idx) # when indexing is not linearly selected\n",
    "    y= sorted(list(enumerate(similarity[idx])), key=lambda x:x[1], reverse=False) #Sort similarity with indices\n",
    "    top_20= y[1:20]  # Select top 20 similar elements from index 1 (inclusive) to 20 (exclusive)\n",
    "    \n",
    "    jobs=[] # try to store value in list\n",
    "    for idx in top_20:\n",
    "        jobs.append(df.iloc[idx[0]].Title) # storing all 20 similar title in a list one by one\n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:55:16.632912Z",
     "iopub.status.busy": "2024-04-18T08:55:16.632667Z",
     "iopub.status.idle": "2024-04-18T08:55:16.660671Z",
     "shell.execute_reply": "2024-04-18T08:55:16.659749Z",
     "shell.execute_reply.started": "2024-04-18T08:55:16.632891Z"
    }
   },
   "outputs": [],
   "source": [
    "recommendation('medic front offic officeteam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-18T08:56:17.221788Z",
     "iopub.status.busy": "2024-04-18T08:56:17.221390Z",
     "iopub.status.idle": "2024-04-18T08:56:19.177442Z",
     "shell.execute_reply": "2024-04-18T08:56:19.176411Z",
     "shell.execute_reply.started": "2024-04-18T08:56:17.221758Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating simple website for this recommendation system\n",
    "# need to pickle 2 things\n",
    "import pickle\n",
    "pickle.dump(df, open('df.pkl','wb')) # binary mode as write-binary mode(wb)\n",
    "pickle.dump(similarity,open('similarity.pkl','wb')) \n",
    "# created these 2 pickle files 'df.pkl' and 'similarity.pkl' can be used later in website\n",
    "# these 2 files 'df.pkl' and 'similarity.pkl'  are being formed in our Output folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "40\n",
    "Web apps. brings us to deploying machine learning models as web applications. The traditional approach is to wrap the API via the use of web frameworks such as Django and Flask. A much simpler approach is to use a low-code solution such as Streamlit to create a web app. Streamlit is an open-source Python library that makes it easy to create and share web apps for machine learning and data science. \n",
    "\n",
    "!pip install streamlit\n",
    "import streamlit as pd\n",
    "import pandas as pd\n",
    "import pickle \n",
    "\n",
    "df = pickle.load(open('df.pkl', 'rb'))  # binary mode as read-binary mode(rb) \n",
    "df = pickle.load(open('similarity.pkl', 'rb'))\n",
    "\n",
    "# streamlit will help in creating website\n",
    "# web app \n",
    "st.title("
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 174180,
     "sourceId": 393357,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30683,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
